{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59763b1b-17c3-44a7-94a2-cebc1f9a3d21",
   "metadata": {},
   "source": [
    "# 大模型（Qwen2.5_Coder_3B） 指令微调教程\n",
    "\n",
    "## 环境准备\n",
    "\n",
    "本教程可在 [AutoDL](https://www.autodl.com/home) 的 4090 GPU 实例上运行。\n",
    "\n",
    "\n",
    "## 教程内容\n",
    "\n",
    "本教程将介绍以下内容:\n",
    "\n",
    "0. [AutoDL配置](#GPU实例) - 如何启动相应配置的GPU实例\n",
    "1. [安装依赖库](#Install) - 如何安装python依赖包\n",
    "2. [模型准备](#Model) - 如何下载和初始化模型\n",
    "3. [数据准备](#Data) - 如何准备和处理训练数据\n",
    "4. [模型训练](#Train) - 如何训练和优化模型\n",
    "5. [模型保存](#Save) - 如何保存训练结果\n",
    "6. [模型推理](#Inference) - 如何使用训练好的模型进行推理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391c6fb-9a4a-4f53-885d-cad1556978da",
   "metadata": {},
   "source": [
    "## 0.AutoDL配置\n",
    "- **为什么选择 AutoDL？**： 相对于其他云服务器厂商，AutoDL卡相对便宜很多，而且操作相对简单，上手成本很低。\n",
    "- **如何配置？**： GPU: RTX 4090(24GB) * 1。  镜像： PyTorch  2.3.0  -->  Python  3.12(ubuntu22.04)  -->  CUDA  12.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cc9b2-701e-4b99-be48-ca938bb8364d",
   "metadata": {},
   "source": [
    "## 1.安装依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f3b2a5c-5178-4c89-8d2d-38000b0da78d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: unsloth in /root/miniconda3/lib/python3.12/site-packages (2025.3.17)\n",
      "Requirement already satisfied: unsloth_zoo>=2025.3.14 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (2025.3.15)\n",
      "Requirement already satisfied: torch>=2.4.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (2.6.0)\n",
      "Requirement already satisfied: xformers>=0.0.27.post2 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.0.29.post3)\n",
      "Requirement already satisfied: bitsandbytes in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.45.3)\n",
      "Requirement already satisfied: triton>=3.0.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (3.2.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (23.2)\n",
      "Requirement already satisfied: tyro in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.9.17)\n",
      "Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (4.49.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (2.21.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (4.67.1)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (5.9.8)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.43.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (1.26.4)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (1.5.2)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.15.2)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.15.0)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.29.3)\n",
      "Requirement already satisfied: hf_transfer in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.1.9)\n",
      "Requirement already satisfied: diffusers in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.32.2)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/lib/python3.12/site-packages (from unsloth) (0.21.0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/miniconda3/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (0.3.6)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface_hub->unsloth) (4.12.1)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.12/site-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.12/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/lib/python3.12/site-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n",
      "Requirement already satisfied: rich in /root/miniconda3/lib/python3.12/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n",
      "Requirement already satisfied: cut_cross_entropy in /root/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.3.14->unsloth) (25.1.1)\n",
      "Requirement already satisfied: pillow in /root/miniconda3/lib/python3.12/site-packages (from unsloth_zoo>=2025.3.14->unsloth) (10.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /root/miniconda3/lib/python3.12/site-packages (from diffusers->unsloth) (8.6.1)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in /root/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in /root/miniconda3/lib/python3.12/site-packages (from tyro->unsloth) (4.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth) (1.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/lib/python3.12/site-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=2.4.0->unsloth) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth) (2025.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: modelscope==1.9.0 in /root/miniconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: addict in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.4.0)\n",
      "Requirement already satisfied: attrs in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (23.2.0)\n",
      "Collecting datasets<=2.13.0,>=2.8.0 (from modelscope==1.9.0)\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/17/d8/f808e32ed7fa86617b9ac7a37b7dcff894c839108c4871cc33ffc4e65b7d/datasets-2.13.0-py3-none-any.whl (485 kB)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (0.8.1)\n",
      "Requirement already satisfied: filelock>=3.3.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (3.14.0)\n",
      "Requirement already satisfied: gast>=0.2.2 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (0.6.0)\n",
      "Requirement already satisfied: ms-swift in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (1.26.4)\n",
      "Requirement already satisfied: oss2 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.19.1)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.2.3)\n",
      "Requirement already satisfied: Pillow>=6.2.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (10.3.0)\n",
      "Requirement already satisfied: pyarrow!=9.0.0,>=6.0.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (19.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.25 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.32.3)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (1.15.2)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (69.5.1)\n",
      "Requirement already satisfied: simplejson>=3.3.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (3.20.1)\n",
      "Requirement already satisfied: sortedcontainers>=1.5.9 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.4.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (2.1.0)\n",
      "Requirement already satisfied: yapf in /root/miniconda3/lib/python3.12/site-packages (from modelscope==1.9.0) (0.43.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]>=2021.11.1->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (0.29.3)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.1->modelscope==1.9.0) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope==1.9.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope==1.9.0) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.25->modelscope==1.9.0) (2024.2.2)\n",
      "Requirement already satisfied: accelerate in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (1.5.2)\n",
      "Requirement already satisfied: diffusers>=0.18.0 in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (0.32.2)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (0.42.1)\n",
      "Requirement already satisfied: matplotlib in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (3.9.0)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (3.9.1)\n",
      "Requirement already satisfied: peft>=0.5.0 in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (0.15.0)\n",
      "Requirement already satisfied: rouge in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (1.0.1)\n",
      "Requirement already satisfied: safetensors in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (0.5.3)\n",
      "Requirement already satisfied: tensorboard in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (2.16.2)\n",
      "Requirement already satisfied: transformers>=4.33 in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (4.49.0)\n",
      "Requirement already satisfied: transformers-stream-generator in /root/miniconda3/lib/python3.12/site-packages (from ms-swift->modelscope==1.9.0) (0.0.5)\n",
      "Requirement already satisfied: crcmod>=1.7 in /root/miniconda3/lib/python3.12/site-packages (from oss2->modelscope==1.9.0) (1.7)\n",
      "Requirement already satisfied: pycryptodome>=3.4.7 in /root/miniconda3/lib/python3.12/site-packages (from oss2->modelscope==1.9.0) (3.22.0)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /root/miniconda3/lib/python3.12/site-packages (from oss2->modelscope==1.9.0) (2.16.5)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /root/miniconda3/lib/python3.12/site-packages (from oss2->modelscope==1.9.0) (2.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->modelscope==1.9.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->modelscope==1.9.0) (2025.1)\n",
      "Requirement already satisfied: platformdirs>=3.5.1 in /root/miniconda3/lib/python3.12/site-packages (from yapf->modelscope==1.9.0) (3.10.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /root/miniconda3/lib/python3.12/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.9.0) (0.10.0)\n",
      "Requirement already satisfied: cryptography>=3.0.0 in /root/miniconda3/lib/python3.12/site-packages (from aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.9.0) (42.0.5)\n",
      "Requirement already satisfied: importlib-metadata in /root/miniconda3/lib/python3.12/site-packages (from diffusers>=0.18.0->ms-swift->modelscope==1.9.0) (8.6.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.12/site-packages (from diffusers>=0.18.0->ms-swift->modelscope==1.9.0) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets<=2.13.0,>=2.8.0->modelscope==1.9.0) (4.12.1)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from peft>=0.5.0->ms-swift->modelscope==1.9.0) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.13.0 in /root/miniconda3/lib/python3.12/site-packages (from peft>=0.5.0->ms-swift->modelscope==1.9.0) (2.6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/miniconda3/lib/python3.12/site-packages (from transformers>=4.33->ms-swift->modelscope==1.9.0) (0.21.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->ms-swift->modelscope==1.9.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->ms-swift->modelscope==1.9.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->ms-swift->modelscope==1.9.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->ms-swift->modelscope==1.9.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.12/site-packages (from matplotlib->ms-swift->modelscope==1.9.0) (3.1.2)\n",
      "Requirement already satisfied: click in /root/miniconda3/lib/python3.12/site-packages (from nltk->ms-swift->modelscope==1.9.0) (8.1.8)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.12/site-packages (from nltk->ms-swift->modelscope==1.9.0) (1.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (1.64.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (3.20.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /root/miniconda3/lib/python3.12/site-packages (from tensorboard->ms-swift->modelscope==1.9.0) (3.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in /root/miniconda3/lib/python3.12/site-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.9.0) (1.16.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft>=0.5.0->ms-swift->modelscope==1.9.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard->ms-swift->modelscope==1.9.0) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/lib/python3.12/site-packages (from importlib-metadata->diffusers>=0.18.0->ms-swift->modelscope==1.9.0) (3.21.0)\n",
      "Requirement already satisfied: pycparser in /root/miniconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->modelscope==1.9.0) (2.21)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.21.0\n",
      "    Uninstalling datasets-2.21.0:\n",
      "      Successfully uninstalled datasets-2.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "trl 0.15.2 requires datasets>=2.21.0, but you have datasets 2.13.0 which is incompatible.\n",
      "unsloth-zoo 2025.3.15 requires datasets>=2.16.0, but you have datasets 2.13.0 which is incompatible.\n",
      "unsloth 2025.3.17 requires datasets>=2.16.0, but you have datasets 2.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.13.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting datasets==2.21.0\n",
      "  Using cached http://mirrors.aliyun.com/pypi/packages/72/b3/33c4ad44fa020e3757e9b2fad8a5de53d9079b501e6bbc45bdd18f82f893/datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /root/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (3.11.14)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (0.29.3)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.12/site-packages (from datasets==2.21.0) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.21.2->datasets==2.21.0) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==2.21.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==2.21.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==2.21.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets==2.21.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.16.0)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.13.0\n",
      "    Uninstalling datasets-2.13.0:\n",
      "      Successfully uninstalled datasets-2.13.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "modelscope 1.9.0 requires datasets<=2.13.0,>=2.8.0, but you have datasets 2.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.21.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: addict in /root/miniconda3/lib/python3.12/site-packages (2.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting vllm\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c4/9d/64e107313a19327b049a2267871cceb9b0415f79ee5c00dc360099f929e8/vllm-0.8.1-cp38-abi3-manylinux1_x86_64.whl (265.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.3/265.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cachetools (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/72/76/20fa66124dbe6be5cafeb312ece67de6b61dd91a0247d1ea13db4ebb33c2/cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.12/site-packages (from vllm) (5.9.8)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.26.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/lib/python3.12/site-packages (from vllm) (4.67.1)\n",
      "Collecting blake3 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/0e/35182a7cf5ee2fb8f49296f34c2880bf99318df4fccb2c294485d4bc1bbb/blake3-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.0/375.0 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting py-cpuinfo (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e0/a9/023730ba63db1e494a271cb018dcd361bd2c917ba7004c3e49d5daf795a2/py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: transformers>=4.48.2 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (4.49.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.21.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.12/site-packages (from vllm) (3.20.3)\n",
      "Collecting fastapi>=0.115.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b3/5d/4d8bbb94f0dbc22732350c06965e40740f4a92ca560e90bb566f4f73af41/fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /root/miniconda3/lib/python3.12/site-packages (from vllm) (3.11.14)\n",
      "Collecting openai>=1.52.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/42/de/b42ddabe211411645105ae99ad93f4f3984f53be7ced2ad441378c27f62e/openai-1.67.0-py3-none-any.whl (580 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.2/580.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic>=2.9 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f4/3c/8cc1cc84deffa6e25d2d0c688ebb80635dfdbf1dbea3e30c541c8cf4d860/pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: prometheus_client>=0.18.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: pillow in /root/miniconda3/lib/python3.12/site-packages (from vllm) (10.3.0)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/27/72/0824c18f3bc75810f55dacc2dd933f6ec829771180245ae3cc976195dec0/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl (19 kB)\n",
      "Collecting tiktoken>=0.6.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1b/40/da42522018ca496432ffd02793c3a72a739ac04c3794a4914570c9bb2925/tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting lm-format-enforcer<0.11,>=0.10.11 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/06/cb/bf172960241842e953b3354247f792aae2fc5221552a0741a1c98f35b6f7/lm_format_enforcer-0.10.11-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting outlines==0.1.11 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/13/b4/99ea4a122bef60e3fd6402d19665aff1f928e0daf8fac3044d0b73f72003/outlines-0.1.11-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting lark==1.2.2 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2d/00/d90b10b962b4277f5e64a78b6609968859ff86889f5b898c1a778c06ec00/lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xgrammar==0.1.16 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f0/4b/94c5801b458d0840c906944a376c50ea3128e98e7819421e246a47d7dd2d/xgrammar-0.1.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.10 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (4.12.1)\n",
      "Collecting filelock>=3.16.1 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting partial-json-parser (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8c/ee/a9476f01f27c74420601be208c6c2c0dd3486681d515e9d765931b89851c/partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyzmq in /root/miniconda3/lib/python3.12/site-packages (from vllm) (26.0.3)\n",
      "Collecting msgspec (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/ef/c5422ce8af73928d194a6606f8ae36e93a52fd5e8df5abd366903a5ca8da/msgspec-0.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.6/213.6 kB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gguf==0.10.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1b/e4/c5f9bd71840ae9afb7e2b7c285ba209f2ef5e9cd83885f8c596c551d3026/gguf-0.10.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /root/miniconda3/lib/python3.12/site-packages (from vllm) (8.6.1)\n",
      "Collecting mistral_common>=1.5.4 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/80/7a/421819257cd642b33d71819e2ff259fb019a49ea48e830e5a32558c52cb7/mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /root/miniconda3/lib/python3.12/site-packages (from vllm) (6.0.1)\n",
      "Requirement already satisfied: six>=1.16.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (1.16.0)\n",
      "Collecting setuptools>=74.1.1 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/40/50/bc3d02829a3babd70b7f1414c93cf6acd198976f0469a07d0e7b813c5002/setuptools-77.0.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: einops in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.8.1)\n",
      "Collecting compressed-tensors==0.9.2 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bb/6e/dc0a80ce14802344e3f4d0520285e8773b83ec2fd864e7cab886718f55a9/compressed_tensors-0.9.2-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting depyf==0.18.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e7/d8/efc291d5c69a9905515055d23977643dd0d482ebfeb0dbabef1947ee75d8/depyf-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting cloudpickle (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7e/e8/64c37fadfc2816a7701fa8a6ed8d87327c7d54eacfbfb6edab14a2f2be75/cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Collecting watchfiles (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2b/b4/9396cc61b948ef18943e7c85ecfa64cf940c88977d882da57147f62b34b1/watchfiles-1.0.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.2/452.2 kB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-json-logger in /root/miniconda3/lib/python3.12/site-packages (from vllm) (2.0.7)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.12/site-packages (from vllm) (1.15.2)\n",
      "Collecting ninja (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6b/35/a8e38d54768e67324e365e2a41162be298f51ec93e6bd4b18d237d7250d8/ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numba==0.60.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8b/41/ac11cf33524def12aa5bd698226ae196a1185831c05ed29dc0c56eaa308b/numba-0.60.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ray>=2.43.0 (from ray[cgraph]>=2.43.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a1/52/99604614000eb5c12e46a3c423c3078494ce032451ac12d8a573cbffb710/ray-2.43.0-cp312-cp312-manylinux2014_x86_64.whl (67.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==2.6.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (2.6.0)\n",
      "Collecting torchaudio==2.6.0 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ed/aa/9082e715a673dd8e22b6a60cec7f301e897406023672b2090f8bcd8a5959/torchaudio-2.6.0-cp312-cp312-manylinux1_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision==0.21.0 in /root/miniconda3/lib/python3.12/site-packages (from vllm) (0.21.0)\n",
      "Collecting xformers==0.0.29.post2 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/37/ac/1aca7e44c93876dbda00e80f79c0bda78bc65e236c68ceb2fc6b26f77df5/xformers-0.0.29.post2-cp312-cp312-manylinux_2_28_x86_64.whl (44.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting astor (from depyf==0.18.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: dill in /root/miniconda3/lib/python3.12/site-packages (from depyf==0.18.0->vllm) (0.3.6)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba==0.60.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/00/5f/323c4d56e8401c50185fd0e875fcf06b71bf825a863699be1eb10aa2a9cb/llvmlite-0.43.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting interegular (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c4/01/72d6472f80651673716d1deda2a5bbb633e563ecf94f4479da5519d69d25/interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (3.1.4)\n",
      "Requirement already satisfied: nest_asyncio in /root/miniconda3/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (1.6.0)\n",
      "Collecting diskcache (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: referencing in /root/miniconda3/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (0.35.1)\n",
      "Requirement already satisfied: jsonschema in /root/miniconda3/lib/python3.12/site-packages (from outlines==0.1.11->vllm) (4.22.0)\n",
      "Collecting pycountry (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b1/ec/1fb891d8a2660716aadb2143235481d15ed1cbfe3ad669194690b0604492/pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting airportsdata (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/19/c3/3fc94ab580f50f56a8f68fd4e619730fbc8c079f0028cf37664c1c7411de/airportsdata-20250224-py3-none-any.whl (913 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting outlines_core==0.1.26 (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e2/1d/a36292b6198986bd9c3ff8c24355deb82ed5475403379ee40b5b5473e2e3/outlines_core-0.1.26-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (343 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.2/343.2 kB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (3.3)\n",
      "Requirement already satisfied: fsspec in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/miniconda3/lib/python3.12/site-packages (from torch==2.6.0->vllm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->vllm) (1.3.0)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a0/4b/528ccf7a982216885a1ff4908e886b8fb5f19862d1962f56a3fce2435a70/starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi-cli>=0.0.5 (from fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a1/e6/5daefc851b514ce2287d8f5d358ae4341089185f78f3217a69d0ce3a390c/fastapi_cli-0.0.7-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /root/miniconda3/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.27.0)\n",
      "Collecting jinja2 (from outlines==0.1.11->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-multipart>=0.0.18 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/45/58/38b5afbc1a800eeea951b9285d3912613f2603bdf897a4ab0f4bd7f405fc/python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Collecting email-validator>=2.0.0 (from fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d7/ee/bf0adb559ad3c786f12bcbc9296b3f5675f529199bef03e2df281fa1fadb/email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Collecting uvicorn>=0.12.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/61/14/33a3a1352cfa71812a3a21e8c9bfb83f60b0011f5e36f2b1399d51928209/uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /root/miniconda3/lib/python3.12/site-packages (from lm-format-enforcer<0.11,>=0.10.11->vllm) (23.2)\n",
      "Collecting opencv-python-headless>=4.0.0 (from mistral_common[opencv]>=1.5.4->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/dd/5c/c139a7876099916879609372bfa513b7f1257f7f1a908b0bdc1c2328241b/opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /root/miniconda3/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /root/miniconda3/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.9.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.52.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/87/67/22728a86ef53589c3720225778f7c5fdb617080e3deaed58b04789418212/jiter-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m351.3/351.3 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /root/miniconda3/lib/python3.12/site-packages (from openai>=1.52.0->vllm) (1.3.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>=2.9->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8d/f0/49129b27c43396581a635d8710dae54a791b17dfc50c70164866bbf865e3/pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing_extensions>=4.10 (from vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/26/9f/ad63fc0248c5379346306f8668cda6e2e2e9c95e01216d2b8ffd9ff037d0/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.12/site-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (8.1.8)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f1/54/65af8de681fa8255402c80eda2a501ba467921d5a7a028c9c22a2c2eedb5/msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.4/401.4 kB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiosignal in /root/miniconda3/lib/python3.12/site-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /root/miniconda3/lib/python3.12/site-packages (from ray>=2.43.0->ray[cgraph]>=2.43.0->vllm) (1.5.0)\n",
      "Collecting cupy-cuda12x (from ray[cgraph]>=2.43.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/cb/fe/661462ae9769e555718c645e5e06194a922a81f75cceb33defdb853cd277/cupy_cuda12x-13.4.0-cp312-cp312-manylinux2014_x86_64.whl (105.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.12/site-packages (from requests>=2.26.0->vllm) (2024.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/miniconda3/lib/python3.12/site-packages (from tiktoken>=0.6.0->vllm) (2024.11.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /root/miniconda3/lib/python3.12/site-packages (from tokenizers>=0.19.1->vllm) (0.29.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.12/site-packages (from transformers>=4.48.2->vllm) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/miniconda3/lib/python3.12/site-packages (from aiohttp->vllm) (1.18.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/lib/python3.12/site-packages (from importlib_metadata->vllm) (3.21.0)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/68/1b/e0a87d256e40e8c888847551b20a017a6b98139178505dc7ffb96f04e954/dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7f/fc/5b29fea8cee020515ca82cc68e3b8e1e34bb19a3535ad854cac9257b414c/typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rich-toolkit>=0.11.1 (from fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7e/1b/1c2f43af46456050b27810a7a013af8a7e12bc545a0cdc00eb0df55eb769/rich_toolkit-0.13.2-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.12/site-packages (from httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /root/miniconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->fastapi[standard]>=0.115.0->vllm) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.12/site-packages (from jinja2->outlines==0.1.11->vllm) (2.1.5)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.12/site-packages (from jsonschema->outlines==0.1.11->vllm) (0.18.1)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f7/d8/b644c44acc1368938317d76ac991c9bba1166311880bcc0ac297cb9d6bd7/httptools-0.6.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.8/510.8 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/06/a7/b4e6a19925c900be9f98bec0a75e6e8f79bb53bdeb891916609ab3958967/uvloop-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/14/8f/aa61f528fba38578ec553c145857a181384c72b98156f858ca5c8e82d9d3/websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastrlock>=0.5 (from cupy-cuda12x->ray[cgraph]>=2.43.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/80/07/cdecb7aa976f34328372f1c4efd6c9dc1b039b3cc8d3f38787d640009a25/fastrlock-0.8.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich>=13.7.1 in /root/miniconda3/lib/python3.12/site-packages (from rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (13.9.4)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm)\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.11.1->fastapi-cli>=0.0.5->fastapi-cli[standard]>=0.0.5; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Installing collected packages: py-cpuinfo, fastrlock, blake3, websockets, uvloop, uvicorn, typing_extensions, shellingham, setuptools, python-multipart, python-dotenv, pycountry, partial-json-parser, opencv-python-headless, ninja, msgspec, msgpack, llvmlite, lark, jiter, jinja2, interegular, httptools, gguf, filelock, dnspython, diskcache, cupy-cuda12x, cloudpickle, cachetools, astor, annotated-types, airportsdata, watchfiles, tiktoken, starlette, pydantic-core, numba, email-validator, depyf, typer, rich-toolkit, pydantic, prometheus-fastapi-instrumentator, xformers, torchaudio, ray, outlines_core, openai, mistral_common, lm-format-enforcer, fastapi-cli, fastapi, xgrammar, outlines, compressed-tensors, vllm\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.12.1\n",
      "    Uninstalling typing_extensions-4.12.1:\n",
      "      Successfully uninstalled typing_extensions-4.12.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.5.1\n",
      "    Uninstalling setuptools-69.5.1:\n",
      "      Successfully uninstalled setuptools-69.5.1\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.14.0\n",
      "    Uninstalling filelock-3.14.0:\n",
      "      Successfully uninstalled filelock-3.14.0\n",
      "  Attempting uninstall: xformers\n",
      "    Found existing installation: xformers 0.0.29.post3\n",
      "    Uninstalling xformers-0.0.29.post3:\n",
      "      Successfully uninstalled xformers-0.0.29.post3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "modelscope 1.9.0 requires datasets<=2.13.0,>=2.8.0, but you have datasets 2.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed airportsdata-20250224 annotated-types-0.7.0 astor-0.8.1 blake3-1.0.4 cachetools-5.5.2 cloudpickle-3.1.1 compressed-tensors-0.9.2 cupy-cuda12x-13.4.0 depyf-0.18.0 diskcache-5.6.3 dnspython-2.7.0 email-validator-2.2.0 fastapi-0.115.11 fastapi-cli-0.0.7 fastrlock-0.8.3 filelock-3.18.0 gguf-0.10.0 httptools-0.6.4 interegular-0.3.3 jinja2-3.1.6 jiter-0.9.0 lark-1.2.2 llvmlite-0.43.0 lm-format-enforcer-0.10.11 mistral_common-1.5.4 msgpack-1.1.0 msgspec-0.19.0 ninja-1.11.1.3 numba-0.60.0 openai-1.67.0 opencv-python-headless-4.11.0.86 outlines-0.1.11 outlines_core-0.1.26 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.1.0 py-cpuinfo-9.0.0 pycountry-24.6.1 pydantic-2.10.6 pydantic-core-2.27.2 python-dotenv-1.0.1 python-multipart-0.0.20 ray-2.43.0 rich-toolkit-0.13.2 setuptools-77.0.1 shellingham-1.5.4 starlette-0.46.1 tiktoken-0.9.0 torchaudio-2.6.0 typer-0.15.2 typing_extensions-4.12.2 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.8.1 watchfiles-1.0.4 websockets-15.0.1 xformers-0.0.29.post2 xgrammar-0.1.16\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install unsloth\n",
    "!pip install modelscope==1.9.0\n",
    "!pip install datasets==2.21.0\n",
    "!pip install addict\n",
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca4b3b-2a1f-4d50-8b03-6ca840d67976",
   "metadata": {},
   "source": [
    "## 2.模型准备\n",
    "### 2.1下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f32e0b57-67be-4be1-8056-85e650f043dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /autodl-fs/data/models/Qwen2.5-Coder-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 09:56:02,050 - modelscope - INFO - Target directory already exists, skipping creation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./models/Qwen2.5-Coder-3B-Instruct'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelscope.hub.snapshot_download import snapshot_download\n",
    "\n",
    "model_name = \"unsloth/Qwen2.5-Coder-3B-Instruct\"\n",
    "local_dir = \"./models/Qwen2.5-Coder-3B-Instruct\"\n",
    "snapshot_download(model_name, local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0532717-9517-493b-b32c-ac0f6a6fafd9",
   "metadata": {},
   "source": [
    "### 2.2 模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22e9cbbd-ba39-4a98-bac4-bea6c16b5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.17: Fast Qwen2 patching. Transformers: 4.49.0. vLLM: 0.8.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.643 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bdac95b2664c4698df7fbe0f4c81d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# 基础配置参数\n",
    "max_seq_length = 2048 # 最大序列长度\n",
    "dtype = None # 自动检测数据类型\n",
    "load_in_4bit = True # 使用4位量化以减少内存使用\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载预训练模型和分词器\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = local_dir, # \"unsloth/Qwen2.5-Coder-32B-Instruct\", # 选择Qwen2.5 3B指令模型\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # LoRA秩,控制可训练参数数量\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # 需要训练的目标模块\n",
    "    lora_alpha = 16, # LoRA缩放因子\n",
    "    lora_dropout = 0, # LoRA dropout率\n",
    "    bias = \"none\", # 是否训练偏置项\n",
    "    use_gradient_checkpointing = \"unsloth\", # 使用梯度检查点节省显存\n",
    "    random_state = 3407, # 随机数种子\n",
    "    use_rslora = False, # 是否使用稳定版LoRA\n",
    "    loftq_config = None, # LoftQ配置\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6687581-0407-4e4e-a24a-a69a67f9235b",
   "metadata": {},
   "source": [
    "### 2.3 未经过训练的模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7fa227a4-9144-4212-952a-57ca31f08360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未经过监督微调的模型输出:  system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "How many r's are in strawberry?\n",
      "assistant\n",
      "There are 3 r's in the word \"strawberry\".\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# 应用聊天模板\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"user\", \"content\": \"How many r's are in strawberry?\"}\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# 配置生成参数\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# 将文本转换为输入张量\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 使用标准的 generate 方法生成输出\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "# 解码输出\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"未经过监督微调的模型输出: \", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8fcd1e-29f1-4947-861c-f5cd330e8093",
   "metadata": {},
   "source": [
    "## 3. 数据准备\n",
    "### 3.1 本地PC下载\n",
    "datasets 是 Hugging Face 提供的用于加载和处理各种数据集的库。AutoDL上无法直接访问 Hugging Face。 因此数据集 \"mlabonne/FineTome-100k\" 需要在本地PC下载后，从AutoDL的“文件存储”上传到你所使用的实例存储位置。\n",
    "\n",
    "本地可以科学上网后，安装 pip install datasets 后，然后运行下面代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "981f151e-24e1-4326-aa3c-1fa3347c89be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach 'mlabonne/FineTome-100k' on the Hub (LocalEntryNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 下载数据集\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlabonne/FineTome-100k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m dataset\u001b[38;5;241m.\u001b[39msave_to_disk(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./datasets/FineTome-100k\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m数据集已成功保存到 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:2061\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2044\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2045\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a metric script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2049\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_metric\u001b[39m(\n\u001b[1;32m   2050\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   2051\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2052\u001b[0m     process_id: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2053\u001b[0m     num_process: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   2054\u001b[0m     cache_dir: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2055\u001b[0m     experiment_id: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2056\u001b[0m     keep_in_memory: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2057\u001b[0m     download_config: Optional[DownloadConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2058\u001b[0m     download_mode: Optional[Union[DownloadMode, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2059\u001b[0m     revision: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Version]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2060\u001b[0m     trust_remote_code: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m-> 2061\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_init_kwargs,\n\u001b[1;32m   2062\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Metric:\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a `datasets.Metric`.\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \n\u001b[1;32m   2065\u001b[0m \u001b[38;5;124;03m    <Deprecated version=\"2.5.0\">\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m   2117\u001b[0m         \u001b[38;5;66;03m# Ignore equivalent warnings to the one already issued\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:1781\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1780\u001b[0m     download_config \u001b[38;5;241m=\u001b[39m DownloadConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_kwargs)\n\u001b[0;32m-> 1781\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[1;32m   1782\u001b[0m download_config\u001b[38;5;241m.\u001b[39mextract_compressed_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m download_config\u001b[38;5;241m.\u001b[39mforce_extract \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:1663\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCachedMetricModuleFactory\u001b[39;00m(_MetricModuleFactory):\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m \u001b[38;5;124;03m    Get the module of a metric that has been loaded once already and cached.\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m \u001b[38;5;124;03m    The script that is loaded from the cache is the most recent one with a matching name.\u001b[39;00m\n\u001b[1;32m   1658\u001b[0m \n\u001b[1;32m   1659\u001b[0m \u001b[38;5;124;03m    <Deprecated version=\"2.5.0\">\u001b[39;00m\n\u001b[1;32m   1660\u001b[0m \n\u001b[1;32m   1661\u001b[0m \u001b[38;5;124;03m    Use the new library 🤗 Evaluate instead: https://huggingface.co/docs/evaluate\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \n\u001b[0;32m-> 1663\u001b[0m \u001b[38;5;124;03m    </Deprecated>\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m     \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the new library 🤗 Evaluate instead: https://huggingface.co/docs/evaluate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1667\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1668\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1669\u001b[0m         name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1670\u001b[0m         dynamic_modules_path: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1671\u001b[0m     ):\n\u001b[1;32m   1672\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/datasets/load.py:1550\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1545\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1546\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires you to execute the dataset script in that\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1547\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m repo on your local machine. Make sure you have read the code there to avoid malicious use, then\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1548\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m set the option `trust_remote_code=True` to remove this error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1549\u001b[0m         )\n\u001b[0;32m-> 1550\u001b[0m _check_library_imports(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, library_imports\u001b[38;5;241m=\u001b[39mlibrary_imports)\n\u001b[1;32m   1551\u001b[0m module_path, \u001b[38;5;28mhash\u001b[39m \u001b[38;5;241m=\u001b[39m _load_importable_file(\n\u001b[1;32m   1552\u001b[0m     dynamic_modules_path\u001b[38;5;241m=\u001b[39mdynamic_modules_path,\n\u001b[1;32m   1553\u001b[0m     module_namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1554\u001b[0m     subdirectory_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhash\u001b[39m,\n\u001b[1;32m   1555\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1556\u001b[0m )\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# make the new module to be noticed by the import system\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach 'mlabonne/FineTome-100k' on the Hub (LocalEntryNotFoundError)"
     ]
    }
   ],
   "source": [
    "# 下载数据集 (此段代码本地PC)\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
    "dataset.save_to_disk(\"./datasets/FineTome-100k\")\n",
    "print(f\"数据集已成功保存到 {local_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efbbf0-67b1-466c-bb0d-4aa1df3aa64c",
   "metadata": {},
   "source": [
    "如果不能科学上网，无法下载对应数据集，我这里也提供了依据下载好的。网盘链接: https://pan.baidu.com/s/1MoVWFoEacQ4_Mu-SVaNkHg?pwd=3mp3 提取码: 3mp3 \n",
    "\n",
    "随后需要将下载的数据上传到AutoDL 对应的位子 \"./datasets/FineTome-100k\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fede037-bc69-498e-85c3-b36f5dbb5b13",
   "metadata": {},
   "source": [
    "### 3.2 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba329d7-4d6a-4078-bb46-f4f1b7c2f074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "\n",
    "# 从本地路径加载数据集\n",
    "dataset_path = \"./datasets/FineTome-100k\"\n",
    "dataset = load_from_disk(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9537470-4c62-4bd2-98e8-00bb3a05ac49",
   "metadata": {},
   "source": [
    "### 3.3 格式转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea3bc118-0685-423f-b0ff-9a65fec25824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': 'How do astronomers determine the original '\n",
      "                               'wavelength of light emitted by a celestial '\n",
      "                               'body at rest, which is necessary for measuring '\n",
      "                               'its speed using the Doppler effect?',\n",
      "                    'role': 'user'},\n",
      "                   {'content': 'Astronomers make use of the unique spectral '\n",
      "                               'fingerprints of elements found in stars. These '\n",
      "                               'elements emit and absorb light at specific, '\n",
      "                               'known wavelengths, forming an absorption '\n",
      "                               'spectrum. By analyzing the light received from '\n",
      "                               'distant stars and comparing it to the '\n",
      "                               'laboratory-measured spectra of these elements, '\n",
      "                               'astronomers can identify the shifts in these '\n",
      "                               'wavelengths due to the Doppler effect. The '\n",
      "                               'observed shift tells them the extent to which '\n",
      "                               'the light has been redshifted or blueshifted, '\n",
      "                               'thereby allowing them to calculate the speed '\n",
      "                               'of the star along the line of sight relative '\n",
      "                               'to Earth.',\n",
      "                    'role': 'assistant'}],\n",
      " 'score': 5.025244235992432,\n",
      " 'source': 'WebInstructSub_axolotl',\n",
      " 'text': '<|im_start|>system\\n'\n",
      "         'You are Qwen, created by Alibaba Cloud. You are a helpful '\n",
      "         'assistant.<|im_end|>\\n'\n",
      "         '<|im_start|>user\\n'\n",
      "         'How do astronomers determine the original wavelength of light '\n",
      "         'emitted by a celestial body at rest, which is necessary for '\n",
      "         'measuring its speed using the Doppler effect?<|im_end|>\\n'\n",
      "         '<|im_start|>assistant\\n'\n",
      "         'Astronomers make use of the unique spectral fingerprints of elements '\n",
      "         'found in stars. These elements emit and absorb light at specific, '\n",
      "         'known wavelengths, forming an absorption spectrum. By analyzing the '\n",
      "         'light received from distant stars and comparing it to the '\n",
      "         'laboratory-measured spectra of these elements, astronomers can '\n",
      "         'identify the shifts in these wavelengths due to the Doppler effect. '\n",
      "         'The observed shift tells them the extent to which the light has been '\n",
      "         'redshifted or blueshifted, thereby allowing them to calculate the '\n",
      "         'speed of the star along the line of sight relative to '\n",
      "         'Earth.<|im_end|>\\n'}\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "import pprint\n",
    "\n",
    "\n",
    "# 配置分词器使用qwen-2.5对话模板\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"格式化对话数据的函数\n",
    "    Args:\n",
    "        examples: 包含对话列表的字典\n",
    "    Returns:\n",
    "        包含格式化文本的字典\n",
    "    \"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "# 标准化数据集格式\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "\n",
    "# 查看第5条对话的结构\n",
    "pprint.pprint(dataset[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b3c2c-f2fa-4776-b283-8c45cb69429d",
   "metadata": {},
   "source": [
    "## 4. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca09e383-dbb5-4af4-84c0-a35ff87fd15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4090. Max memory = 23.643 GB.\n",
      "6.217 GB of memory reserved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 29,933,568/3,000,000,000 (1.00% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.654100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.773600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.653500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.531300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.809000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.586000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.711200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.985600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.858700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.687100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.536300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.914900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.832400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.487100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.801100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.927700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.880900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.623100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.555600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.560700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.562100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.736900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.099300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.518600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.790600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.603800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.936500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.544800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.773200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.632300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.658600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.757700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.524800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.481600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.433200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.853500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.1242 seconds used for training.\n",
      "2.57 minutes used for training.\n",
      "Peak reserved memory = 7.834 GB.\n",
      "Peak reserved memory for training = 1.617 GB.\n",
      "Peak reserved memory % of max memory = 33.135 %.\n",
      "Peak reserved memory for training % of max memory = 6.839 %.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "output_dir = \"./outputs/01_outputs\"    # 检查点是训练过程中的一个快照，\n",
    "                                       # 它记录了模型在某个特定训练步骤的状态，包括模型的权重、优化器的状态等。\n",
    "\n",
    "\n",
    "# 配置训练器\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=4,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # 每个设备的批次大小\n",
    "        gradient_accumulation_steps=4,  # 梯度累积步数\n",
    "        warmup_steps=5,  # 预热步数\n",
    "        max_steps=100,  # 最大训练步数\n",
    "        learning_rate=2e-4,  # 学习率\n",
    "        fp16=not is_bfloat16_supported(),  # 是否使用fp16\n",
    "        bf16=is_bfloat16_supported(),  # 是否使用bf16\n",
    "        logging_steps=1,  # 日志记录间隔\n",
    "        optim=\"paged_adamw_8bit\",  # 优化器\n",
    "        weight_decay=0.01,  # 权重衰减\n",
    "        lr_scheduler_type=\"linear\",  # 学习率调度器\n",
    "        seed=3407,  # 随机种子\n",
    "        output_dir=output_dir,  # 输出目录\n",
    "        report_to=\"none\",  # 不使用外部日志工具\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "# 设置仅对助手回复部分计算损失\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")\n",
    "\n",
    "# 查看输入文本\n",
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])\n",
    "\n",
    "# 查看标签掩码\n",
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])\n",
    "\n",
    "# 获取GPU信息\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# 显示训练统计信息\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4e176-96d2-47cb-af61-88dcbbba95b1",
   "metadata": {},
   "source": [
    "## 5. 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5468242-d34a-4002-bd36-27ebc5cde059",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = output_dir + '/lora_model'\n",
    "\n",
    "# 本地保存模型和分词器\n",
    "model.save_pretrained(save_path)  # 保存模型权重\n",
    "tokenizer.save_pretrained(save_path)  # 保存分词器\n",
    "\n",
    "\n",
    "if False:\n",
    "    # 在线保存到 HuggingFace Hub\n",
    "    model.push_to_hub(\"your_name/lora_model\", token = \"...\") # 上传模型到Hub\n",
    "    tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # 上传分词器到Hub\n",
    "\n",
    "\n",
    "if False:\n",
    "    # 使用标准Hugging Face接口加载\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        save_path,  # 模型路径\n",
    "        load_in_4bit=load_in_4bit,  # 4bit加载\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(save_path)  # 加载分词器\n",
    "\n",
    "\n",
    "if False: \n",
    "    # 把模型以 16 位格式合并保存到 model 目录，同时传入分词器和保存方法\n",
    "    model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3d8e3-88b4-4577-9efb-e4143792b127",
   "metadata": {},
   "source": [
    "## 6.模型推理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd33a9fd-9136-4ce2-a03c-a4baccff99f5",
   "metadata": {},
   "source": [
    "### 6.1 使用监督微调的 LoRA 进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "362612e1-35f6-4c88-9c31-9be879681efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用监督微调的 LoRA 模型输出:  system\n",
      "你是一个知识渊博、友好的助手，能准确回答各种问题。\n",
      "user\n",
      "How many r's are in strawberry?\n",
      "assistant\n",
      "The word \"strawberry\" contains 3 'r's.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "\n",
    "# 禁用 peft 的 UserWarning（关键修改）\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"peft\")\n",
    "\n",
    "# 定义 SYSTEM_PROMPT\n",
    "SYSTEM_PROMPT = \"你是一个知识渊博、友好的助手，能准确回答各种问题。\"\n",
    "\n",
    "# 加载 LoRA 权重\n",
    "model = PeftModel.from_pretrained(model, save_path)\n",
    "\n",
    "# 应用聊天模板\n",
    "text = tokenizer.apply_chat_template([\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": \"How many r's are in strawberry?\"}\n",
    "], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# 配置生成参数\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "\n",
    "# 将文本转换为输入张量\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 使用标准的 generate 方法生成输出\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "# 解码输出\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"使用监督微调的 LoRA 模型输出: \", output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad6f4f-15e1-495c-a233-2fa62c0d0116",
   "metadata": {},
   "source": [
    "### 6.2 配置推理用的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "63d78e4e-9571-499f-8c4e-bef289c2882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): PeftModelForCausalLM(\n",
       "          (base_model): LoraModel(\n",
       "            (model): PeftModelForCausalLM(\n",
       "              (base_model): LoraModel(\n",
       "                (model): PeftModelForCausalLM(\n",
       "                  (base_model): LoraModel(\n",
       "                    (model): Qwen2ForCausalLM(\n",
       "                      (model): Qwen2Model(\n",
       "                        (embed_tokens): Embedding(151936, 2048, padding_idx=151665)\n",
       "                        (layers): ModuleList(\n",
       "                          (0-35): 36 x Qwen2DecoderLayer(\n",
       "                            (self_attn): Qwen2Attention(\n",
       "                              (q_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (k_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (v_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (o_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (rotary_emb): LlamaRotaryEmbedding()\n",
       "                            )\n",
       "                            (mlp): Qwen2MLP(\n",
       "                              (gate_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (up_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (down_proj): lora.Linear4bit(\n",
       "                                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "                                (lora_dropout): ModuleDict(\n",
       "                                  (default): Identity()\n",
       "                                )\n",
       "                                (lora_A): ModuleDict(\n",
       "                                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "                                )\n",
       "                                (lora_B): ModuleDict(\n",
       "                                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                                )\n",
       "                                (lora_embedding_A): ParameterDict()\n",
       "                                (lora_embedding_B): ParameterDict()\n",
       "                                (lora_magnitude_vector): ModuleDict()\n",
       "                              )\n",
       "                              (act_fn): SiLU()\n",
       "                            )\n",
       "                            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                          )\n",
       "                        )\n",
       "                        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "                        (rotary_emb): LlamaRotaryEmbedding()\n",
       "                      )\n",
       "                      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置推理用的分词器\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model)   # 推理模式（仅需一次）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b9c3e-473d-48fc-ba9c-33f653b279c5",
   "metadata": {},
   "source": [
    "### 6.3 准备输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f71cf95-2198-4011-99a1-1b328ac407ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试输入\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"Here is a programming problem for testing:\n",
    "\n",
    "    **Matrix Chain Multiplication Optimization**\n",
    "\n",
    "    ### Problem:\n",
    "    Given a chain of matrices `A1, A2, ..., An`, where the dimensions of Ai are `P[i-1] x P[i]`,\n",
    "    find the optimal parenthesization order that minimizes the total scalar multiplication cost.\n",
    "\n",
    "    **Input:**\n",
    "    1. An array `P` representing dimensions, e.g., P = [10, 20, 30, 40].\n",
    "\n",
    "    **Output:**\n",
    "    1. The optimal parenthesization order (e.g., `(A1 x (A2 x A3))`).\n",
    "    2. The minimum scalar multiplication cost.\n",
    "    3. A comparison to the naive left-to-right multiplication cost.\n",
    "\n",
    "    ### Constraints:\n",
    "    - Use dynamic programming to solve this problem efficiently.\n",
    "    - Provide a solution for P of length up to 10^5 (optional for advanced testing).\n",
    "\n",
    "    ### Example:\n",
    "    Input: P = [10, 20, 30]\n",
    "    Output:\n",
    "    - Optimal order: `(A1 x A2)`\n",
    "    - Minimum cost: 6000\n",
    "    - Naive cost: 6000\n",
    "\n",
    "    Input: P = [10, 20, 30, 40]\n",
    "    Output:\n",
    "    - Optimal order: `((A1 x A2) x A3)`\n",
    "    - Minimum cost: 18000\n",
    "    - Naive cost: 24000\n",
    "\n",
    "    Implement the solution and evaluate it against these criteria.\"\"\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf5e1b-b936-4c2e-a525-a723e798e54d",
   "metadata": {},
   "source": [
    "### 6.4 普通生成输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04db8984-0772-43a5-9888-c44f3fd0f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "普通生成结果：\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Here is a programming problem for testing:\n",
      "\n",
      "    **Matrix Chain Multiplication Optimization**\n",
      "\n",
      "    ### Problem:\n",
      "    Given a chain of matrices `A1, A2, ..., An`, where the dimensions of Ai are `P[i-1] x P[i]`,\n",
      "    find the optimal parenthesization order that minimizes the total scalar multiplication cost.\n",
      "\n",
      "    **Input:**\n",
      "    1. An array `P` representing dimensions, e.g., P = [10, 20, 30, 40].\n",
      "\n",
      "    **Output:**\n",
      "    1. The optimal parenthesization order (e.g., `(A1 x (A2 x A3))`).\n",
      "    2. The minimum scalar multiplication cost.\n",
      "    3. A comparison to the naive left-to-right multiplication cost.\n",
      "\n",
      "    ### Constraints:\n",
      "    - Use dynamic programming to solve this problem efficiently.\n",
      "    - Provide a solution for P of length up to 10^5 (optional for advanced testing).\n",
      "\n",
      "    ### Example:\n",
      "    Input: P = [10, 20, 30]\n",
      "    Output:\n",
      "    - Optimal order: `(A1 x A2)`\n",
      "    - Minimum cost: 6000\n",
      "    - Naive cost: 6000\n",
      "\n",
      "    Input: P = [10, 20, 30, 40]\n",
      "    Output:\n",
      "    - Optimal order: `((A1 x A2) x A3)`\n",
      "    - Minimum cost: 18000\n",
      "    - Naive cost: 24000\n",
      "\n",
      "    Implement the solution and evaluate it against these criteria.\n",
      "assistant\n",
      "To solve the Matrix Chain Multiplication problem using dynamic programming, we can use memoization to optimize the solution. The idea is to create a DP table where `dp[i][j]` represents the minimum scalar multiplication cost to multiply subchain `P[i] * P[i+1] * ... * P[j]\n"
     ]
    }
   ],
   "source": [
    "# 普通生成（删除冗余的 FastLanguageModel.for_inference）\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_new_tokens=64,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")\n",
    "\n",
    "print(\"普通生成结果：\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20fb6a9-588c-4331-820e-05b2a25eaa30",
   "metadata": {},
   "source": [
    "### 6.5 流式生成输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ff061-2d8d-41c6-ae3f-cc85e0c795a5",
   "metadata": {},
   "source": [
    "**使用 TextStreamer 进行流式生成:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fc2c0f4-977b-4b49-a9e3-a01e435f398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "流式生成结果：\n",
      "To solve the Matrix Chain Multiplication problem using dynamic programming, we need to determine the most efficient way to parenthesize the product of matrices. Here's a step-by-step implementation in Python, along with comments explaining the logic and comparisons:\n",
      "\n",
      "```python\n",
      "def matrix_chain_order(P):\n",
      "    n = len(P) - 1  # Number of matrices\n",
      "\n",
      "    # Create a DP table to store the minimum cost and corresponding bracketing\n",
      "    dp = [[0] * n for _ in range(n)]\n",
      "    bracketing = [[' '] * n for _ in range(n)]\n",
      "\n",
      "    # Fill the DP table\n",
      "    for L in range(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
       "             13, 151645,    198, 151644,    872,    198,   8420,    374,    264,\n",
       "          15473,   3491,    369,   7497,   1447,    262,   3070,   6689,  28525,\n",
       "          58712,   1693,  57739,  56177,    262,  16600,  22079,    510,    262,\n",
       "          16246,    264,   8781,    315,  35195,   1565,     32,     16,     11,\n",
       "            362,     17,     11,  60353,   1527,   7808,   1380,    279,  15336,\n",
       "            315,  55986,    525,   1565,     47,    989,     12,     16,     60,\n",
       "            856,    393,    989,     60,  12892,    262,   1477,    279,  22823,\n",
       "          37940,   6375,   2022,   1973,    429,  21187,   4756,    279,   2790,\n",
       "          17274,  46444,   2783,    382,    262,   3070,   2505,     25,   1019,\n",
       "            262,    220,     16,     13,   1527,   1334,   1565,     47,     63,\n",
       "          14064,  15336,     11,    384,   1302,   2572,    393,    284,    508,\n",
       "             16,     15,     11,    220,     17,     15,     11,    220,     18,\n",
       "             15,     11,    220,     19,     15,  29562,    262,   3070,   5097,\n",
       "             25,   1019,    262,    220,     16,     13,    576,  22823,  37940,\n",
       "           6375,   2022,   1973,    320,     68,   1302,   2572,  48651,     32,\n",
       "             16,    856,    320,     32,     17,    856,    362,     18,    593,\n",
       "             63,   4292,    262,    220,     17,     13,    576,   8028,  17274,\n",
       "          46444,   2783,    624,    262,    220,     18,     13,    362,  12313,\n",
       "            311,    279,  49665,   2115,   4686,   6701,  46444,   2783,    382,\n",
       "            262,  16600,  87473,    510,    262,    481,   5443,   8741,  15473,\n",
       "            311,  11625,    419,   3491,  29720,    624,    262,    481,  39565,\n",
       "            264,   6291,    369,    393,    315,   3084,    705,    311,    220,\n",
       "             16,     15,     61,     20,    320,  12807,    369,  10847,   7497,\n",
       "           3593,    262,  16600,  13383,    510,    262,   5571,     25,    393,\n",
       "            284,    508,     16,     15,     11,    220,     17,     15,     11,\n",
       "            220,     18,     15,    921,    262,   9258,    510,    262,    481,\n",
       "          16554,   2861,   1973,     25,  48651,     32,     16,    856,    362,\n",
       "             17,  49237,    262,    481,  30925,   2783,     25,    220,     21,\n",
       "             15,     15,     15,    198,    262,    481,  12812,    533,   2783,\n",
       "             25,    220,     21,     15,     15,     15,    271,    262,   5571,\n",
       "             25,    393,    284,    508,     16,     15,     11,    220,     17,\n",
       "             15,     11,    220,     18,     15,     11,    220,     19,     15,\n",
       "            921,    262,   9258,    510,    262,    481,  16554,   2861,   1973,\n",
       "             25,   1565,   1188,     32,     16,    856,    362,     17,      8,\n",
       "            856,    362,     18,  49237,    262,    481,  30925,   2783,     25,\n",
       "            220,     16,     23,     15,     15,     15,    198,    262,    481,\n",
       "          12812,    533,   2783,     25,    220,     17,     19,     15,     15,\n",
       "             15,    271,    262,  31075,    279,   6291,    323,  15442,    432,\n",
       "           2348,   1493,  12890,     13, 151645,    198, 151644,  77091,    198,\n",
       "           1249,  11625,    279,  11631,  28525,  58712,   1693,   3491,   1667,\n",
       "           8741,  15473,     11,    582,   1184,    311,   8253,    279,   1429,\n",
       "          11050,   1616,    311,  37940,  26887,    279,   1985,    315,  35195,\n",
       "             13,   5692,    594,    264,   3019,  14319,  29208,   8129,    304,\n",
       "          13027,     11,   3156,    448,   6042,  25021,    279,  12218,    323,\n",
       "          35495,   1447,  73594,  12669,    198,    750,   6172,  30583,   7869,\n",
       "           5304,    982,    262,    308,    284,   2422,   5304,      8,    481,\n",
       "            220,     16,    220,    671,   5624,    315,  35195,    271,    262,\n",
       "            671,   4230,    264,  31757,   1965,    311,   3553,    279,   8028,\n",
       "           2783,    323,  12159,  31642,    287,    198,    262,  11329,    284,\n",
       "           4318,     15,     60,    353,    308,    369,    716,    304,   2088,\n",
       "           1445,   5563,    262,  31642,    287,    284,  31827,  42877,    353,\n",
       "            308,    369,    716,    304,   2088,   1445,  27771,    262,    671,\n",
       "          21979,    279,  31757,   1965,    198,    262,    369,    444,    304,\n",
       "           2088,      7]], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 流式生成（复用已准备的 inputs）\n",
    "from transformers import TextStreamer\n",
    "\n",
    "print(\"\\n流式生成结果：\")\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)  # 跳过输入提示\n",
    "\n",
    "model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72065f14-3cdf-4382-8f44-5fb05f56f05b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
